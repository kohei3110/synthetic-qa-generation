{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Document Intelligenceを使用して複雑なPDFからQnA合成データセットを生成する\n",
    "\n",
    "### 概要\n",
    "PDFを3つの部分に分けて処理します。\n",
    "\n",
    "- **混合ページ（画像とテキストが適切に混在）** - Azure AI Document Intelligenceでドキュメントを読み取った後、図のタグ内の画像説明をマルチモーダルLLMで要約されたテキストに置き換えます。（しばしば画像説明は空白か短いキャプションのみです。）\n",
    "- **Text-heavy** - テキストが多いPDFは、Azure AI Document IntelligenceやUnstructuredのようなツールキットを使用せずに、オープンソースで処理できます。\n",
    "- **Image-heavy** - 画像が多いPDFは、ページ全体を画像に変換し、GPT-4oのようなマルチモーダルLLMに各ページを要約させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read & Preprocess PDF file\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFを個々のページに分割する\n",
    "テストのためにPDFドキュメントの一部のみを使用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Distributed training on Cloud, Language: Japanese, Language Code: ja\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from util.common_utils import get_language_code\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "raw_data_dir = \"../raw_data\"\n",
    "splitted_raw_data_dir = \"splitted_raw_data\"\n",
    "file_path = f\"{raw_data_dir}/pdf/azure-ai-search-overview.pdf\"\n",
    "\n",
    "DOMAIN = \"Distributed training on Cloud\"\n",
    "LANGUAGE = \"Japanese\" # You can change your language here. e.g., \"Korean\", \"English\", \"Chinese\"\n",
    "LANGUAGE_CODE = get_language_code(LANGUAGE)\n",
    "print(f\"Domain: {DOMAIN}, Language: {LANGUAGE}, Language Code: {LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストのためにPDFドキュメントの一部のみを使用します。ページが多い場合や部分的な処理が必要な場合は、一部のページのみを切り取って保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open the first PDF document\n",
    "doc1 = fitz.open(file_path)\n",
    "split_pages = [(5, 25)]\n",
    "\n",
    "for idx, s in enumerate(split_pages):\n",
    "    # Create a new empty PDF document\n",
    "    doc2 = fitz.open()\n",
    "\n",
    "    # Insert the first 2 pages of doc1 into doc2\n",
    "    doc2.insert_pdf(doc1, from_page=s[0], to_page=s[1])\n",
    "\n",
    "    # Save the modified document\n",
    "    doc2.save(f\"{raw_data_dir}/part{idx}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主にテキストで構成されたページ、主に画像で構成されたページ、およびテキストと画像が混在するページを区別します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'splitted_raw_data' and its contents have been deleted.\n",
      "### PDF Content Analysis Result:\n",
      "Mixed pages: [0]\n"
     ]
    }
   ],
   "source": [
    "from util.common_utils import delete_folder_and_make_folder\n",
    "from util.preprocess import analyze_pdf_page_content, split_pdf\n",
    "\n",
    "file_path = f\"{raw_data_dir}/part0.pdf\"\n",
    "analyzed_pdf_result = analyze_pdf_page_content(file_path)\n",
    "delete_folder_and_make_folder(splitted_raw_data_dir)    \n",
    "\n",
    "print(\"### PDF Content Analysis Result:\")\n",
    "for content_type, pages in analyzed_pdf_result.items():\n",
    "    print(f\"{content_type} pages: {pages}\")\n",
    "    split_pdf(file_path, f\"{splitted_raw_data_dir}/{content_type}.pdf\", pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOC_INTELLIGENCE_KEY\")\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=doc_intelligence_endpoint, \n",
    "    credential=AzureKeyCredential(doc_intelligence_key),\n",
    "    headers={\"x-ms-useragent\":\"sample-code-figure-understanding/1.0.0\"},\n",
    ")\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=aoai_api_key,  \n",
    "    api_version=aoai_api_version,\n",
    "    base_url=f\"{aoai_api_endpoint}/openai/deployments/{aoai_deployment_name}\",\n",
    "    max_retries=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース1: 混合ページ（画像とテキストが適切に混在）の場合\n",
    "Azure AI Document Intelligenceでドキュメントを読み取った後、`img`タグ内の画像説明をマルチモーダルLLMで要約されたテキストに置き換えます。（しばしば画像説明は空白か短いキャプションのみです。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ドキュメントの分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdf_mixed_tmp' and its contents have been deleted.\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    pdf_mixed_path = f\"{splitted_raw_data_dir}/Mixed.pdf\"\n",
    "\n",
    "    with open(pdf_mixed_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", \n",
    "            output_content_format=ContentFormat.MARKDOWN \n",
    "        )\n",
    "\n",
    "    result = poller.result()\n",
    "    md_content = result.content\n",
    "\n",
    "    #### Updates the content of the figure description (empty content or caption) with the image summary text generated by gpt-4o.\n",
    "    from util.preprocess import (\n",
    "        image_complexity, is_bounding_box_larger_than, crop_image_from_file, \n",
    "        understand_image_with_gpt, update_figure_description\n",
    "    )\n",
    "    output_folder = \"pdf_mixed_tmp\"\n",
    "    delete_folder_and_make_folder(output_folder)\n",
    "    language = LANGUAGE\n",
    "    max_tokens = 1024\n",
    "    input_file_path = file_path\n",
    "\n",
    "    if result.figures:\n",
    "        print(\"Figures:\")\n",
    "        for idx, figure in enumerate(result.figures):\n",
    "            figure_content = \"\"\n",
    "            img_description = \"\"\n",
    "            \n",
    "            for i, span in enumerate(figure.spans):\n",
    "                figure_content += md_content[span.offset:span.offset + span.length]\n",
    "\n",
    "            # Note: figure bounding regions currently contain both the bounding region of figure caption and figure body\n",
    "            if figure.caption:\n",
    "                caption_region = figure.caption.bounding_regions\n",
    "                for region in figure.bounding_regions:\n",
    "                    if region not in caption_region:\n",
    "                        boundingbox = (\n",
    "                                region.polygon[0],  # x0 (left)\n",
    "                                region.polygon[1],  # y0 (top)\n",
    "                                region.polygon[4],  # x1 (right)\n",
    "                                region.polygon[5]   # y1 (bottom)\n",
    "                            )\n",
    "\n",
    "                        if is_bounding_box_larger_than(boundingbox):\n",
    "                            cropped_image = crop_image_from_file(pdf_mixed_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                            if image_complexity(cropped_image)[0] == \"Complex\":\n",
    "                                # Get the base name of the file\n",
    "                                base_name = os.path.basename(input_file_path)\n",
    "                                # Remove the file extension\n",
    "                                file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                                output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                                cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "\n",
    "                                cropped_image.save(cropped_image_filename)\n",
    "                                print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                                try: \n",
    "                                    image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                                except openai.BadRequestError as e:\n",
    "                                    print(f\"BadRequestError: {e}\")\n",
    "                                    image_summarization = \"\"\n",
    "                                img_description += image_summarization\n",
    "\n",
    "                                print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                            else:\n",
    "                                print(f'simple image at idx {idx}')\n",
    "\n",
    "            else:\n",
    "                for region in figure.bounding_regions:\n",
    "\n",
    "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                    boundingbox = (\n",
    "                            region.polygon[0],  # x0 (left)\n",
    "                            region.polygon[1],  # y0 (top\n",
    "                            region.polygon[4],  # x1 (right)\n",
    "                            region.polygon[5]   # y1 (bottom)\n",
    "                        )\n",
    "\n",
    "                    if is_bounding_box_larger_than(boundingbox):                    \n",
    "\n",
    "                        cropped_image = crop_image_from_file(input_file_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                        if image_complexity(cropped_image)[0] == \"Complex\":\n",
    "                            # Get the base name of the file\n",
    "                            base_name = os.path.basename(input_file_path)\n",
    "                            # Remove the file extension\n",
    "                            file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                            output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                            cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "                            # cropped_image_filename = f\"data/cropped/image_{idx}.png\"\n",
    "                            cropped_image.save(cropped_image_filename)\n",
    "\n",
    "                            try:\n",
    "                                image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                            except openai.BadRequestError as e:\n",
    "                                print(f\"BadRequestError: {e}\")\n",
    "                                image_summarization = \"\"\n",
    "                            img_description += image_summarization\n",
    "                            print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                        else:\n",
    "                            print(f'simple image at idx {idx}')\n",
    "\n",
    "            \n",
    "            md_content = update_figure_description(md_content, img_description, idx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混合ページのチャンクを生成します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits (mixed case): 1\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import re\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\n",
    "            r'<!-- PageNumber=\"\\d+\" -->',\n",
    "            r\"\\n\\n\",\n",
    "            r\"\\n\",\n",
    "            \" \",\n",
    "            \".\",\n",
    "            \"\",\n",
    "        ],   \n",
    "        is_separator_regex = True,    \n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "\n",
    "    mixed_chunks = text_splitter.split_text(md_content)\n",
    "    print(\"Length of splits (mixed case): \" + str(len(mixed_chunks)))\n",
    "else:\n",
    "    mixed_chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース2: テキストが多い場合\n",
    "テキストが多いPDFは、Azure AI Document IntelligenceやUnstructuredのようなツールキットを使用せずに、オープンソースで処理できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Text\" in analyzed_pdf_result:\n",
    "    from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "    from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "    pdf_text_path = f\"{splitted_raw_data_dir}/Text.pdf\"\n",
    "    loader = PyMuPDFLoader(pdf_text_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200, \n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    for idx, chunk in enumerate(text_chunks):\n",
    "        print(f\"Chunk {idx}\\n{chunk}\")\n",
    "        print(\"=\"*80)\n",
    "        if idx == 2:\n",
    "            break\n",
    "\n",
    "    text_chunks = [d.page_content for d in text_chunks]\n",
    "    print(\"Length of splits (text-heay case): \" + str(len(text_chunks)))\n",
    "else:\n",
    "    text_chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース3: 画像が多い場合\n",
    "画像が多いPDFは、ページ全体を画像に変換し、GPT-4oのようなマルチモーダルLLMに各ページを要約させます。\n",
    "\n",
    "### 画像の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Image\" in analyzed_pdf_result:\n",
    "    import fitz\n",
    "    from glob import glob\n",
    "\n",
    "    image_dir = \"./pdf_image_tmp\"\n",
    "    delete_folder_and_make_folder(image_dir) \n",
    "\n",
    "    pdf_image_path = f\"{splitted_raw_data_dir}/Image.pdf\"\n",
    "    doc = fitz.open(pdf_image_path)\n",
    "    clip_x, clip_y = 10, 10\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        x, y, w, h = page.rect\n",
    "        clip = fitz.Rect(x+clip_x, y+clip_y, w-clip_x, h-clip_y)\n",
    "        page.set_cropbox(clip)\n",
    "        pix = page.get_pixmap()\n",
    "        pix.save(f\"{image_dir}/page_{i:03d}.jpg\")\n",
    "\n",
    "    images = sorted(glob(os.path.join(image_dir, \"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "max_tokens = 1024\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=\"2024-05-01-preview\",\n",
    "    azure_deployment=\"gpt-4o\"                       \n",
    ")\n",
    "\n",
    "human_prompt_main = f\"Given image, give a concise summary in {LANGUAGE}. Don't insert any XML tag such as <text> and </text> when answering.\"\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": human_prompt_main\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 μs, sys: 1e+03 ns, total: 5 μs\n",
      "Wall time: 8.11 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if \"Image\" in analyzed_pdf_result:\n",
    "    from util.preprocess import encode_image_base64\n",
    "    #images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "    base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "    image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 8})\n",
    "    image_summaries = remove_short_sentences(image_summaries)\n",
    "    print(\"Length of image_summaries (image-heavy case): \" + str(len(image_summaries)))\n",
    "else:\n",
    "    image_summaries = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q&Aペアの構築\n",
    "----\n",
    "\n",
    "### オプション1.\n",
    "azure-ai-generativeパッケージを活用します。このパッケージのQADataGeneratorクラスを使用すると、QnAの合成質問を簡単に生成できます。ただし、このクラスをそのまま使用するとカスタムプロンプトを使用できないという欠点があるため、これを継承してCustomQADataGeneratorクラスを作成しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.qa import CustomQADataGenerator\n",
    "model_config = {\n",
    "    \"deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = CustomQADataGenerator(model_config=model_config, templates_dir=f\"./prompt_template/{LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "import os\n",
    "from azure.ai.generative.synthetic.qa import QAType\n",
    "concurrency = 6  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "#qa_type = QAType.CONVERSATION\n",
    "qa_type = QAType.LONG_ANSWER\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated QAs\n"
     ]
    }
   ],
   "source": [
    "input_batch = mixed_chunks + text_chunks + image_summaries\n",
    "results = await asyncio.gather(*[generate_async(text) for text in input_batch], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "token_usage = Counter()\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "    token_usage += result[\"token_usage\"]\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('どのような大きな強みがあるのですか？',\n",
       "  '大きな強みには、ベクトルおよび非ベクトル（テキスト）のインデックス作成とクエリのサポートが含まれます。特に、ベクトル類似性検索を使用することで、検索語句が完全に一致しない場合でも、意味的に類似した情報を見つけることが可能です。また、ハイブリッド検索を使用することで、キーワード検索とベクトル検索の長所を最大限に活かすことができます。\\n'),\n",
       " ('セマンティックランク付けとスコアリングプロファイルはどのように機能しますか？',\n",
       "  'セマンティックランク付けとスコアリングプロファイルは、クエリのランク付けと関連性のチューニングを行います。クエリ構文では、用語ブーストとフィールドの優先順位付けがサポートされており、これにより検索結果の精度を向上させることができます。\\n'),\n",
       " ('Azureの統合機能にはどのようなものがありますか？',\n",
       "  'Azureの統合機能には、インデックス層でのデータ統合（クローラー）や、コンテンツのテキストとベクターを検索可能にするためのAzure AI統合が含まれます。また、Microsoft Entraセキュリティによる信頼された接続や、インターネットなしのシナリオでのプライベート接続用のAzure Private Linkも提供されています。')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オプション2.\n",
    "別のツールキットを使用せずに、QnAデータセットを作成するための一連のコードをすべて記述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from util.qa_pair import get_qna_prompt_template, QAPair\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1024,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name                    \n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QAPair)\n",
    "prompt = get_qna_prompt_template(LANGUAGE)\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "\n",
    "for doc in mixed_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in text_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in image_summaries:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 ms, sys: 15.7 ms, total: 49.4 ms\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qa_pair = chain.batch(input_batch, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ファインチューニング用にjsonl形式で保存\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from util.common_utils import convert_to_oai_format, convert_to_jsonl_format, save_jsonl\n",
    "\n",
    "output_dir = './dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "system_prompt_msg = f\"\"\"You are the SME (Subject Matter Expert) in {DOMAIN}. Please answer the questions accurately. If the question is in {LANGUAGE}, write your answer in {LANGUAGE}.\"\"\"\n",
    "\n",
    "save_filename = \"imagenet-training-summary\"\n",
    "oai_qa_pair = convert_to_oai_format(question_answer_list, system_prompt_msg=system_prompt_msg)\n",
    "\n",
    "#save_jsonl(qa_pair, f\"{output_dir}/{save_filename}.jsonl\")\n",
    "save_jsonl(oai_qa_pair, f\"{output_dir}/{save_filename}-oai.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AI Studio 評価用にjsonl形式で保存\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': 'どのような大きな強みがあるのですか？', 'ground_truth': '大きな強みには、ベクトルおよび非ベクトル（テキスト）のインデックス作成とクエリのサポートが含まれます。特に、ベクトル類似性検索を使用することで、検索語句が完全に一致しない場合でも、意味的に類似した情報を見つけることが可能です。また、ハイブリッド検索を使用することで、キーワード検索とベクトル検索の長所を最大限に活かすことができます。\\n'}, {'question': 'セマンティックランク付けとスコアリングプロファイルはどのように機能しますか？', 'ground_truth': 'セマンティックランク付けとスコアリングプロファイルは、クエリのランク付けと関連性のチューニングを行います。クエリ構文では、用語ブーストとフィールドの優先順位付けがサポートされており、これにより検索結果の精度を向上させることができます。\\n'}, {'question': 'Azureの統合機能にはどのようなものがありますか？', 'ground_truth': 'Azureの統合機能には、インデックス層でのデータ統合（クローラー）や、コンテンツのテキストとベクターを検索可能にするためのAzure AI統合が含まれます。また、Microsoft Entraセキュリティによる信頼された接続や、インターネットなしのシナリオでのプライベート接続用のAzure Private Linkも提供されています。'}]\n"
     ]
    }
   ],
   "source": [
    "for qa in question_answer_list:\n",
    "    qa_pair = convert_to_jsonl_format(qa)\n",
    "    print(qa_pair)\n",
    "    save_jsonl(qa_pair, f\"{output_dir}/{save_filename}-eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クリーンアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {splitted_raw_data_dir} pdf_image_tmp pdf_mixed_tmp outputs_tmp images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
